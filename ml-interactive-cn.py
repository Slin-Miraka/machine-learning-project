# -*- coding: utf-8 -*-
"""
Created on Tue Jan 26 22:58:23 2021

@author: ShingFoon Lin

"""

import streamlit as st
import numpy as np
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import train_test_split
from sklearn import tree
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
from PIL import Image
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import learning_curve



st.title('**æœºå™¨å­¦ä¹ äº¤äº’app (demo)**')

with st.sidebar.header('Welcomeï¼ o(*ï¿£â–½ï¿£*)ãƒ–'):
    dataset_name = st.sidebar.selectbox(
        'é€‰æ‹©ä¸€ä¸ªä½ å–œæ¬¢çš„æ•°æ®é›†',
        ('é¸¢å°¾èŠ±æ•°æ®é›†', 'ä¹³è…ºç™Œæ•°æ®é›†', 'è‘¡è„é…’æ•°æ®é›†')
    )

    classifier_name = st.sidebar.selectbox('é€‰æ‹©åˆ†ç±»å™¨',('å†³ç­–æ ‘','LazyClassifier','KNN', 'SVM', 'Random Forest'))
    

with st.sidebar.header('åŸºç¡€å‚æ•°'):
    parameter_random_state = st.sidebar.slider('éšæœºç§å­ä¸ªæ•°(ç›®çš„ï¼šç»“æœå¤ç°)', 0, 1000, 42, 1)
    split_size = st.sidebar.slider('æ•°æ®æ‹†åˆ†æ¯”ä¾‹ (è®¾ç½®è®­ç»ƒé›†çš„ç™¾åˆ†æ¯”)', 10, 90, 80, 5)
    
def add_parameter_ui():  
    params = dict()
    with st.sidebar.header('æ¨¡å‹å‚æ•°'):
        params['criterion'] = st.sidebar.selectbox('è¯„ä»·æ¯æ¬¡å†³ç­–æ ‘å†³ç­–çš„åˆ†æè´¨é‡çš„æŒ‡æ ‡ï¼Œå³è¡¡é‡ä¸çº¯åº¦çš„æŒ‡æ ‡è¾“å…¥"gini"ä½¿ç”¨åŸºå°¼ç³»æ•°ï¼Œæˆ–è¾“å…¥"entropy" ä½¿ç”¨ä¿¡æ¯å¢ç›Š(Information Gain)',('gini', 'entropy'))
        params['splitter'] = st.sidebar.selectbox('ç¡®å®šæ¯ä¸ªèŠ‚ç‚¹çš„åˆ†æç­–ç•¥è¾“å…¥"best"ä½¿ç”¨æœ€ä½³åˆ†æï¼Œæˆ–è¾“å…¥"random"ä½¿ç”¨æœ€ä½³éšæœºåˆ†æ',('best', 'random'))
        params['max_features'] = st.sidebar.selectbox('åœ¨åšæœ€ä½³åˆ†æçš„æ—¶å€™ï¼Œè€ƒè™‘çš„ç‰¹å¾ä¸ªæ•°', options=['auto', 'sqrt', 'log2'])  
    with st.sidebar.header('æ¨¡å‹ä¿®æ­£å‚æ•°'):
        params['max_depth'] = st.sidebar.slider('æ ‘çš„æœ€å¤§æ·±åº¦(max_depth)',1, 10, 4, 1)
        params['min_samples_split'] = st.sidebar.slider('ä¸€ä¸ªä¸­é—´èŠ‚ç‚¹è¦åˆ†ææ‰€éœ€è¦çš„æœ€å°æ ·æœ¬é‡(min_sample_split)', 1, 10, 4, 1)
        params['min_samples_leaf'] = st.sidebar.slider('ä¸€ä¸ªå¶èŠ‚ç‚¹è¦å­˜åœ¨æ‰€éœ€è¦çš„æœ€å°æ ·æœ¬é‡(min_sample_leaf)', 1, 10, 4, 1)
        return params
 
@st.cache(suppress_st_warning=True)
def get_dataset(name):
    data = None
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        data = datasets.load_iris()
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        data = datasets.load_wine()
    else:
        data = datasets.load_breast_cancer()
    X = data.data
    y = data.target
    return X, y

def get_target_cn(name):
    target = None
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        target = ['å±±é¸¢å°¾','å˜è‰²é¸¢å°¾','ç»´å‰å°¼äºšé¸¢å°¾']
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        target = ['è‘¡è„é…’_1','è‘¡è„é…’_2','è‘¡è„é…’_3']
    else:
        target = ['æ¶æ€§è‚¿ç˜¤','è‰¯æ€§è‚¿ç˜¤']
    return target

def get_feature_cn(name):
    feature = None
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        feature = ['èŠ±è¼é•¿åº¦(cm)','èŠ±è¼å®½åº¦(cm)','èŠ±ç“£é•¿åº¦(cm)','èŠ±ç“£å®½åº¦(cm)']
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        feature = ['é…’ç²¾','è‹¹æœé…¸','ç°','ç°çš„ç¢±æ€§','é•','æ€»é…š','ç±»é»„é…®','éé»„çƒ·ç±»é…šç±»','èŠ±é’ç´ ','é¢œè‰²å¼ºåº¦','è‰²è°ƒ','od280/od315ç¨€é‡Šè‘¡è„é…’','è„¯æ°¨é…¸']
    else:
        feature = ['å¹³å‡åŠå¾„','å¹³å‡çº¹ç†','å¹³å‡å‘¨é•¿','å¹³å‡é¢ç§¯','å¹³å‡å…‰æ»‘åº¦','å¹³å‡ç´§å‡‘åº¦','å¹³å‡å‡¹é™·','å¹³å‡å‡¹ç‚¹','å¹³å‡å¯¹ç§°æ€§','å¹³å‡åˆ†å½¢ç»´åº¦'
                   ,'åŠå¾„è¯¯å·®','çº¹ç†è¯¯å·®','å‘¨é•¿è¯¯å·®','é¢ç§¯è¯¯å·®','å…‰æ»‘åº¦è¯¯å·®','ç´§å‡‘åº¦è¯¯å·®','å‡¹é™·è¯¯å·®','å‡¹ç‚¹è¯¯å·®','å¯¹ç§°æ€§è¯¯å·®','åˆ†å½¢å°ºå¯¸è¯¯å·®'
                   ,'æœ€å·®åŠå¾„','æœ€å·®çº¹ç†','æœ€å·®å‘¨é•¿','æœ€å·®é¢ç§¯','æœ€å·®å¹³æ»‘åº¦','æœ€å·®ç´§å‡‘åº¦','æœ€å·®å‡¹é™·','æœ€å·®å‡¹ç‚¹','æœ€å·®å¯¹ç§°æ€§','æœ€å·®åˆ†å½¢å°ºå¯¸']
    return feature


def get_target_en(name):
    target = None
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        target = datasets.load_iris().target_names
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        target = datasets.load_wine().target_names
    else:
        target = datasets.load_breast_cancer().target_names
    return target

def get_feature_en(name):
    feature = None
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        feature = datasets.load_iris().feature_names
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        feature = datasets.load_wine().feature_names
    else:
        feature = datasets.load_breast_cancer().feature_names
    return feature


T_cn = get_target_cn(dataset_name)
F_cn = get_feature_cn(dataset_name)
T_en = get_target_en(dataset_name)
F_en = get_feature_en(dataset_name)

F_1 = ["æ ‡ç­¾(target)"]

X, y = get_dataset(dataset_name)

df = pd.concat([pd.DataFrame(X,columns = F_cn),pd.DataFrame(y,columns = F_1)],axis = 1)


st.write('**Edit by:** Miraka Lin')
#st.write('**Email:** slin3137@uni.sydney.edu.au')
st.subheader('**1.æ•°æ®ä»‹ç»**')


def data_description(name):
    if name == 'é¸¢å°¾èŠ±æ•°æ®é›†':
        st.write('''é¸¢å°¾èŠ±æ•°æ®é›†æœ€åˆç”±Edgar Anderson æµ‹é‡å¾—åˆ°ï¼Œè€Œååœ¨è‘—åçš„ç»Ÿè®¡å­¦å®¶å’Œç”Ÿç‰©å­¦å®¶R.A Fisheräº1936å¹´å‘è¡¨çš„æ–‡
                 ç« ã€ŒThe use of multiple measurements in taxonomic problemsã€ä¸­è¢«ä½¿ç”¨ï¼Œç”¨å…¶ä½œä¸ºçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear
                 Discriminant Analysisï¼‰çš„ä¸€ä¸ªä¾‹å­ï¼Œè¯æ˜åˆ†ç±»çš„ç»Ÿè®¡æ–¹æ³•ï¼Œä»æ­¤è€Œè¢«ä¼—äººæ‰€çŸ¥ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨å­¦ä¹ è¿™ä¸ªé¢†åŸŸã€‚
                 é¸¢å°¾èŠ±æ•°æ®é›†å…±æ”¶é›†äº†ä¸‰ç±»é¸¢å°¾èŠ±ï¼Œå³Setosaé¸¢å°¾èŠ±ã€Versicolouré¸¢å°¾èŠ±å’ŒVirginicaé¸¢å°¾èŠ±ï¼Œ
                 æ¯ä¸€ç±»é¸¢å°¾èŠ±æ”¶é›†äº†50æ¡æ ·æœ¬è®°å½•ï¼Œå…±è®¡150æ¡ã€‚æ•°æ®é›†åŒ…æ‹¬4ä¸ªå±æ€§ï¼Œåˆ†åˆ«ä¸ºèŠ±è¼çš„é•¿ã€èŠ±è¼çš„å®½ã€èŠ±ç“£çš„é•¿å’ŒèŠ±ç“£çš„å®½ã€‚''')
    elif name == 'è‘¡è„é…’æ•°æ®é›†':
        st.write('''è‘¡è„é…’æ•°æ®é›†åŒ…å«åœ¨æ„å¤§åˆ©çš„ä¸€ä¸ªç‰¹å®šåŒºåŸŸå‡ºäº§çš„è‘¡è„é…’çš„åŒ–å­¦åˆ†æçš„ç»“
                     æœã€‚178ä¸ªæ ·æœ¬ä¸­ä»£è¡¨äº†ä¸‰ç§è‘¡è„é…’ï¼Œæ¯ä¸ªæ ·æœ¬è®°å½•äº†13ç§åŒ–å­¦åˆ†æçš„ç»“æœã€‚''')
    else:
        st.write('''è¯¥æ•°æ®é›†ä¸ºå¨æ–¯åº·æ˜Ÿä¹³è…ºç™Œæ•°æ®é›†ï¼Œæ€»å…±569ä¸ªç—…ä¾‹ï¼Œå…¶ä¸­212ä¸ªæ¶æ€§ï¼Œ357ä¸ªè‰¯æ€§ã€‚
                 æ•°æ®é›†å…±æœ‰10ä¸ªåŸºæœ¬å˜é‡ï¼Œ ä»£è¡¨è‚¿ç˜¤å›¾ç‰‡çš„ç—…ç†å‚æ•°ã€‚æ¯ä¸ªåŸºæœ¬å˜é‡æœ‰ä¸‰ä¸ªç»´åº¦mean, standard error, worst
                 ä»£è¡¨æŸé¡¹å‚æ•°çš„å‡å€¼ï¼Œæ ‡å‡†å·®å’Œæœ€å·®å€¼ï¼Œ å…±è®¡æ˜¯30ä¸ªç‰¹å¾å˜é‡ã€‚''')

data_description(dataset_name)



if st.checkbox('æŸ¥çœ‹æ•°æ®é›†'):
    st.write('å±•ç¤ºæ•°æ®é›†çš„**å‰5è¡Œ**')
    st.write(df.head(5))
    st.write('**æ•°æ®ç»“æ„:**')
    st.write('æ³¨ï¼šæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾åˆ—')
    st.info(df.shape)
    st.write('**æ ‡ç­¾ç±»åˆ«:**')
    st.info(T_cn)
    st.write('**æè¿°æ€§ç»Ÿè®¡**')
    st.write(df.describe())
    
    
###train the model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size,random_state = parameter_random_state)

#build model
params = add_parameter_ui()  

clf = DecisionTreeClassifier(criterion= params['criterion']
                             ,random_state= parameter_random_state
                             ,splitter= params['splitter']
                             ,max_depth = params['max_depth']
                             ,min_samples_split = params['min_samples_split']
                             ,min_samples_leaf = params['min_samples_leaf']
                             )
clf = clf.fit(X_train, y_train)
score_train = clf.score(X_train, y_train)
score_test = clf.score(X_test, y_test)
train_predict = clf.predict(X_train)
test_predict = clf.predict(X_test)

    #C = st.sidebar.slider('C', 0.01, 10.0)
    #params['C'] = C
    
def build_model():  
    st.subheader("**2.åˆæ­¥æ­å»ºæ¨¡å‹**")
    st.write("è°ƒæ•´å·¦ä¾§è¾¹æ çš„å‚æ•°ï¼Œè§‚å¯Ÿæ¨¡å‹å‡†ç¡®ç‡ï¼Œæ··æ·†çŸ©é˜µçš„å˜åŒ–ã€‚")
    st.write('**è®­ç»ƒé›†å‡†ç¡®ç‡**: ', round(score_train, 3))
    #st.write([str(i) + str(j) for i, j in zip(T_cn + T_en)])
    cm_columns = ["é¢„æµ‹ä¸º"+ i for i in T_cn]
    cm_index = ["å®é™…ä¸º"+ i for i in T_cn]
    st.table(pd.DataFrame(confusion_matrix(y_train, train_predict),columns = cm_columns,index = cm_index))
    
    st.write('**æµ‹è¯•é›†å‡†ç¡®ç‡**: ', round(score_test, 3))
    st.table(pd.DataFrame(confusion_matrix(y_test, test_predict),columns = cm_columns,index = cm_index))    
    st.write("**æ··æ·†çŸ©é˜µè§£åº¦**ï¼šè‹¥æ ·æœ¬å®é™…ä¸ºAï¼Œé¢„æµ‹ä¹Ÿä¸ºAï¼Œåˆ™ä¸ºé¢„æµ‹æ­£ç¡®æ ·æœ¬ï¼›è‹¥æ ·æœ¬å®é™…ä¸ºAï¼Œæ¨¡å‹é¢„æµ‹ä¸ºBï¼Œåˆ™ä¸ºé¢„æµ‹é”™è¯¯æ ·æœ¬")
    
    #learning curve
    cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)
    train_sizes = np.linspace(0.1, 1.0, 7)
    
    st.write('**å­¦ä¹ ç‡æ›²çº¿**')
    st.write("é€šè¿‡è§‚å¯Ÿå­¦ä¹ æ›²çº¿äº†è§£æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆçš„æƒ…å†µï¼Œä»è€Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚")
    train_sizes, train_scores, test_scores = learning_curve(clf, X=X, y=y, cv=cv, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fig, ax = plt.subplots(figsize=(6, 5))
    ax.set_xlabel("sample quantity", fontsize=12)
    ax.set_ylabel("accuracy", fontsize=12)
    ax.set_title("model learning curve", fontsize=12)
    ax.grid()
    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1,
                    color="r")
    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1,
                    color="g")
    ax.plot(train_sizes, train_scores_mean, 'o-', color="r",
            label="training score")
    ax.plot(train_sizes, test_scores_mean, 'o-', color="g",
            label="test score")
    ax.legend(loc="best", fontsize=10)
    st.pyplot(fig)
    st.write("è®­ç»ƒä¸€ä¸ªåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†å‡†ç¡®ç‡éƒ½å¾ˆé«˜çš„æ¨¡å‹ï¼")
    
build_model()  
##############################################å†³ç­–æ ‘æ¨¡å‹####################################    

def Decision_Tree_Visualisation():
    #decision tree plot
    dot_data = tree.export_graphviz(clf
                                    ,feature_names= F_cn
                                    ,class_names= T_cn
                                    ,filled = True
                                    ,rounded= True
                                    ,out_file= None)
    
    st.subheader('**3.æ•°æ®å¯è§†åŒ–**')
    st.write('**å†³ç­–æ ‘ ** --å½“å‰å¶å­çš„ä¸ªæ•°:',clf.get_n_leaves())
    st.graphviz_chart(dot_data)
    
    ##Feature importance plot
    feature_column = ["Features","Important_Socre"]
    feature_data = pd.DataFrame([*zip(F_en,clf.feature_importances_)],columns =feature_column)
    feature_data = feature_data.sort_values(by = ["Important_Socre"],ascending=False)
    feature_data = feature_data.loc[feature_data['Important_Socre']>0]
    st.write('**ç”¨äºåˆ†ç±»çš„é‡è¦ç‰¹å¾**'
             #,feature_data.loc[feature_data['Important_Socre']>0]
             )
    #st.info(pd.DataFrame([*zip(F,clf.feature_importances_)]))
    plt.figure(figsize=(6,3))
    #sns.set_style(style="whitegrid")
    #plt.rcParams['font.sans-serif']=['SimHei','Arial']
    #plt.rcParams['axes.unicode_minus'] = False
    #sns.set(font=['SimHei','Arial'], font_scale=0.8)
    sns.set_style('whitegrid',{'font.sans-serif':['simhei','Arial']}) 

    ax = sns.barplot(y="Features", x="Important_Socre", data=feature_data)
    fig = ax.get_figure()
    st.pyplot(fig)
    st.write("ä¸Šè¿°ä¸ºæ¨¡å‹ç”¨äºåˆ†ç±»é€‰æ‹©çš„é‡è¦å˜é‡ï¼Œå¹¶æ›´å…·é‡è¦æ€§ä¾æ¬¡æ’åº")
    st.write("**æŠ±æ­‰ğŸ¥ºï¼Œç”±äºä½œå›¾æ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼Œæ‰€ä»¥åªèƒ½ç”¨è‹±æ–‡å˜é‡è¡¨ç¤º**")
        


def Decision_Boundary_plot():
    ##Decision boundary plot
    
    n_classes = len(np.unique(y))
    color_t = ["red","blue","orange","green"]
    colors = random.sample(color_t, len(np.unique(y)))
    plot_colors = colors
    plot_step = 0.02
    st.subheader('**4.æ¨¡å‹ä¿®æ­£**--å¯¹å†³ç­–æ ‘è¿›è¡Œå‰ªæä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ')
    st.write('**å†³ç­–è¾¹ç•Œå›¾**-- ä½¿ç”¨ä¸€å¯¹åˆ†ç±»é‡è¦ç‰¹å¾')
    st.write('''ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œéœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é™ä½å†³ç­–æ ‘çš„è‡ªç”±åº¦ã€‚å¯ä»¥é€šè¿‡è®¾å®šä¸€äº›å‚æ•°æ¥å®ç°ã€‚
             æœ€å…¸å‹çš„å‚æ•°æ˜¯æ˜¯æ ‘çš„æœ€å¤§æ·±åº¦max_depthï¼Œå‡å°æ ‘çš„æ·±åº¦èƒ½é™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
             è¿˜æœ‰ä¸€äº›å…¶ä»–å‚æ•°ï¼Œå¯ä»¥é™åˆ¶å†³ç­–æ ‘çš„å½¢çŠ¶ï¼šmin_sample_split:åˆ†è£‚å‰èŠ‚ç‚¹å¿…é¡»æœ‰çš„æœ€å°æ ·æœ¬æ•°ï¼Œ
             min_sample_leaf:å¶èŠ‚ç‚¹å¿…é¡»æœ‰çš„æœ€å°æ ·æœ¬æ•°é‡ã€‚''')
    image = Image.open('overfitting.jpg')
    st.image(image
             #, caption='Sunrise by the mountains'
             ,use_column_width=True)
    st.write('è§‚å¯Ÿä¸Šå›¾å¯ä»¥å‘ç°ï¼Œå·¦å›¾æ˜æ˜¾ä¸ºè¿‡æ‹Ÿåˆ--è®­ç»ƒé›†å†³ç­–è¾¹ç•Œä¸ºæ¨¡å‹åˆ’å¼€è¿‡å¤šçš„è¾¹ç•Œï¼Œé™ä½äº†æ¨¡å‹åœ¨æµ‹è¯•é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚')
    feature_column = ["Features","Important_Socre"]
    feature_data = pd.DataFrame([*zip(F_en,clf.feature_importances_)],columns =feature_column)
    feature_data = feature_data.sort_values(by = ["Important_Socre"],ascending=False)
    feature_data = feature_data.loc[feature_data['Important_Socre']>0]
    feature_B = list(feature_data.iloc[:,0])
    x_B = st.selectbox('é€‰æ‹©Xè½´',feature_B)
    y_B = st.selectbox('é€‰æ‹©Yè½´',feature_B)
    
    if x_B == y_B:
        st.write("è¯·é€‰æ‹©ä¸åŒçš„**X**å’Œ**Y**æ¥ç”»å‡ºå†³ç­–è¾¹ç•Œå›¾å§  **!**")
    else:
        fig = plt.figure(figsize=(6,3))
        plt.rcParams['font.sans-serif']=['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        Xj = pd.DataFrame(X,columns = F_en)
        #switch to array
        Xi = Xj.loc[:,(x_B,y_B)].values
        yi = y
        
        #st.write(Xi)
        # Train
        cllf = clf.fit(Xi, yi)
        # Plot the decision boundary
        #plt.subplot(2, 3, pairidx + 1)
        
        x_min, x_max = Xi[:, 0].min() - 1, Xi[:, 0].max() + 1
        y_min, y_max = Xi[:, 1].min() - 1, Xi[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                             np.arange(y_min, y_max, plot_step))
        
        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
    
        Z = cllf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        plt.contourf(xx
                    ,yy
                    ,Z
                    ,cmap=plt.cm.PiYG
                    ,alpha = 0.8)
    
        plt.xlabel(x_B)
        plt.ylabel(y_B)
    
        # Plot the training points
        
        for i, color in zip(range(n_classes), plot_colors):
            idx = np.where(yi == i)
            plt.scatter(Xi[idx, 0]
                        ,Xi[idx, 1]
                        ,c=color
                        ,label= get_target_en(dataset_name)[i]
                        ,cmap=plt.cm.PiYG
                        ,edgecolor='black'
                        #,alpha = 0.3
                        ,s=15)
    
        plt.suptitle('Decision boundary plot')
        plt.legend(loc='lower right', borderpad=0, handletextpad=0)
        plt.axis("tight")
        st.pyplot(fig)
        
##############################################å†³ç­–æ ‘æ¨¡å‹####################################    

Decision_Tree_Visualisation()
Decision_Boundary_plot()